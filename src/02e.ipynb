{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11963cc7",
   "metadata": {},
   "source": [
    "# Market Basket Analysis with Association Rules in Python\n",
    "\n",
    "## Learning Objectives\n",
    "Market basket analysis or affinity analysis is the process of identifying patterns and extracting meaningful insight from transaction sets. Association rules are often used for market basket analysis because they allow us to discover, quantify and analyze the co-occurrence of items within a set of transactions. By the end of this tutorial, you will have learned:\n",
    "\n",
    "+ How to import and explore a transaction set\n",
    "+ How to create Frequent Itemsets\n",
    "+ How to create Association Rules\n",
    "+ How to evaluate Association Rules "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2887bb87",
   "metadata": {},
   "source": [
    "## 1. Collect the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96c17b",
   "metadata": {},
   "source": [
    "The data that we're going to analyze is market basket data collected from a local grocery store over a 30-day period. The data is stored in CSV (Comma  Separated Values) format as follows:\n",
    "```\n",
    "1. citrus fruit,semi-finished bread,margarine,ready soups\n",
    "2. tropical fruit,yogurt,coffee\n",
    "3. whole milk\n",
    "4. pip fruit,yogurt,cream cheese,meat spreads\n",
    "5. other vegetables,whole milk,condensed milk,long life bakery product\n",
    "```\n",
    "Each row in the data represents a set of items purchased by a customer during a store visit, which we refer to as a transaction. As expected, the number of items in each transaction varies so we cannot simply bulk import this data into a tabular data structure such as a pandas DataFrame as-is. Instead, we need to import the data one row at a time.\n",
    "\n",
    "To do this, we first need to import the `reader` object from the `csv` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c664ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff3ec92",
   "metadata": {},
   "source": [
    "Next, we iterate over each line in the input file (*groceries.csv*) and append it to a list called `groceries`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e3b3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "groceries = []\n",
    "with open('groceries.csv', 'r') as csvfile:\n",
    "    csv_reader = reader(csvfile)\n",
    "    for row in csv_reader:\n",
    "        groceries.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836693c2",
   "metadata": {},
   "source": [
    "Let’s preview the first 5 elements in the `groceries` list to make sure that the import worked as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05752d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "groceries[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd79d76",
   "metadata": {},
   "source": [
    "Now that we've imported the transactions into a list, we need to encode them and represent the data in a sparse format before we can generate frequent itemsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32993d81",
   "metadata": {},
   "source": [
    "To transform our data, we first need to import the `TransactionEncoder` class from the `mlxtend.preprocessing` subpackage. The `mlxtend` package provides several functions and objects for preprocesing transaction data, generating frequent itemsets and creating association rules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53e450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791af210",
   "metadata": {},
   "source": [
    "Then we instantiate an object called `encoder` from the `TransactionEncoder` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99669672",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransactionEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f56bc2b",
   "metadata": {},
   "source": [
    "Using the `encoder` object, we call the `fit()` method to extract the unique labels in the transaction set and the `transform()` method to one-hot encode the transactions into a boolean NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9019f8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transactions = encoder.fit(groceries).transform(groceries)\n",
    "transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ede88f",
   "metadata": {},
   "source": [
    "Next, we convert the transactions into a pandas DataFrame. This requires that we first import the `pandas` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5262ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2020e01",
   "metadata": {},
   "source": [
    "Then we pass the boolean array and item names to the pandas `DataFrame()` constructor function to create a new DataFrame called `itemsets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ccf29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "itemsets = pd.DataFrame(transactions, columns = encoder.columns_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d25f52",
   "metadata": {},
   "source": [
    "We can preview the first 5 rows in the `itemsets` DataFrame by calling the `head()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83bab97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "itemsets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a6a2f0",
   "metadata": {},
   "source": [
    "We can also get a concise summary of the structure of the `itemsets` DataFrame by calling the `info()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c5e631",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "itemsets.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2bdb9d",
   "metadata": {},
   "source": [
    "By looking at the `RangeIndex` value of the summary, we can tell that there are 9,835 transactions in the dataset. The `Columns` value tells us that that there are 169 features (or unique items) in the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14e2ae3",
   "metadata": {},
   "source": [
    "## 2. Generate Frequent Itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5426f581",
   "metadata": {},
   "source": [
    "Now that we have our transactions in a compatible format (one-hot encoded pandas DataFrame), let's limit our focus to the frequent itemsets. The `mlxtend.frequent_patterns` subpackage provides three functions for generating frequent itemsets - `apriori`, `fpgrowth` and `fpmax`. All three functions have similar syntax, so we'll limit our illustration to the the `apriori` function. Let's import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947456f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2d03a",
   "metadata": {},
   "source": [
    "The `apriori` function takes several arguments. The first one is the pandas DataFrame of the transactions we wish to analyze. The second is the minimum support threshold of the itemsets we consider frequent. This value specifies how often an itemset must occur in the transaction set in order to warrant our attention. \n",
    "\n",
    "Let’s assume that we only want to focus our attention on itemsets that occur at least $5$ times a day. Given that our data is for $30$ days and our dataset has $9,835$ transactions, this means that we need to set our minimum support threshold to $ 5 \\times \\frac{30}{9835} \\approx 0.015$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db7d690",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(itemsets, min_support = 0.015, use_colnames = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf0c5fb",
   "metadata": {},
   "source": [
    "By default, the `apriori` function returns the numeric column indices of the items in the transaction set. For ease of interpretation, we set the `use_colnames` argument within the function to `True`. This tells the function to return item names instead of integer values. Let's see what we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046f5901",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fab2e83",
   "metadata": {},
   "source": [
    "From the output, we can tell that there are $180$ itemsets with a minimum support value of $0.015$. To get a better sense of which itemsets have the lowest or highest support values, let's sort the data (in descending order of support):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba8e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets.sort_values('support', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5fac31",
   "metadata": {},
   "source": [
    "We see that `{whole milk}`, `{other vegetables}`, `{rolls/buns}`, `{soda}`, and `{yogurt}` are the five most frequently bought items in the store.\n",
    "\n",
    "One of the benefits of working with pandas DataFrames is that we can easily transform and filter our results to meet our needs. For example, let's assume that we are only interested in frequent itemsets with a length greater than $2$. We start by getting the length of the items in the `itemsets` column as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ad4460",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = frequent_itemsets['itemsets'].str.len()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d2d49b",
   "metadata": {},
   "source": [
    "Then we create a logical filter based on the length of the item sets:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8302997b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows = length > 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7609fc56",
   "metadata": {},
   "source": [
    "Finally, we apply the filter to the `frequent_itemsets` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be48d27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets[rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95b1cc5",
   "metadata": {},
   "source": [
    "Now we see the six frequent itemsets with a length greater than $2$.\n",
    "\n",
    "We can also use the `describe()` method of a pandas DataFrame to get a big picture view of the distribution of values in the data. For example, to get a statistical summary of the support values by itemset length, we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93699e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets.groupby(length)['support'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d19add4",
   "metadata": {},
   "source": [
    "The `count` column tells us that the majority of the transactions that are considered frequent are two-item purchases ($101$), while the `mean` and `50%` columns, tell us that transactions with a length of $1$ typically have higher support values than those with a length of $2$ or $3$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad623ab0",
   "metadata": {},
   "source": [
    "In this tutorial we only use the `apriori` frequent itemset generation function. Note that if you want to try out the `fpgrowth` or the `fpmax` functions, you simply need to import them as follows:\n",
    "```python\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "```\n",
    "or\n",
    "```python\n",
    "from mlxtend.frequent_patterns import fpmax\n",
    "```\n",
    "Then you can generate frequent itemsets using the applicable function just as we've done here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca7bb2b",
   "metadata": {},
   "source": [
    "## 3. Create Association Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35760fca",
   "metadata": {},
   "source": [
    "The next step in our market basket analysis process is to create association rules that describe the co-occurrence of itemsets within the transaction set. The `association_rules` function in the `mlxtend.frequent_patterns` subpackage allows us to create these rules. Let's import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de30bca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f32625b6",
   "metadata": {},
   "source": [
    "The `association_rules` function takes several arguments. The first is the frequent itemset. The next is the metric we intend to use to filter the rules for significance. This can either be \"*support*\", \"*confidence*\", \"*lift*\", \"*leverage*\" or \"*conviction*\". \n",
    "\n",
    "Let's assume that we want to limit our focus to rules that have a confidence of `0.25` or more. To do this, we set the `metric` argument to `\"confidence\"` and the `min_threshold` argument to `0.25`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee562710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc7cced1",
   "metadata": {},
   "source": [
    "Let's see what rules were generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4def968e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0c280a5",
   "metadata": {},
   "source": [
    "There are $78$ association rules that meet our criteria. Each rule is made up of an antecedent and a consequent. For each rule, we get metrics that tell us the support of the antecedent and the support of the consequent. We also get metrics that tell us the support, confidence, lift, leverage and conviction of each rule. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24309c60",
   "metadata": {},
   "source": [
    "Because our rules are returned as a pandas DataFrames we can easily transform and filter the data to find what we need. For example, let's say we're only interested in rules that include `'rolls/buns'` in the antecedent. We start by creating a logical expression as a filter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cd1177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a2ec99f",
   "metadata": {},
   "source": [
    "Note that the entries in the `antecedents` and `consequents` columns are of type frozenset, which is why we include the curly braces `{}` around the item names.\n",
    "\n",
    "The next step is to apply the filter to the `rules` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67debed5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad0bab57",
   "metadata": {},
   "source": [
    "We get $1$ rule that matches our criteria. As you can imagine, we can create a similar filter with the consequent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c687d53f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42945c19",
   "metadata": {},
   "source": [
    "This time, we get $3$ rules that match our filter.\n",
    "\n",
    "Note that in the previous two examples, we only matched rules with `'rolls/buns'` alone as the antecedent or the consequent. If our goal is to match all rules with `'rolls/buns'` and any other item in the antecedent for example, we would need to convert the antecedent column to a string, vectorize the string and use the `contains()` method in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee998e24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "860af7f9",
   "metadata": {},
   "source": [
    "There are $5$ rules with `'rolls/buns'` anywhere in the antecedent.\n",
    "\n",
    "We can aslo filter our rules by the length of the antecedent or consequent. For example, to match only rules with an antecedent length more than `1` we do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1606ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "259a358a",
   "metadata": {},
   "source": [
    "We get $16$ rules that match our criteria.\n",
    "\n",
    "We can also filter our rules based on the values in any of the numeric columns. For example, let's assume that we only want to see rules that have a lift of more than `2`, a leverage score more than `0.01` and a conviction score of more than `1.4`. This can be written as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29572d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e529903",
   "metadata": {},
   "source": [
    "There are $3$ rules with a lift of more than `2`, a leverage score more than `0.01` and a conviction score of more than `1.4`.\n",
    "\n",
    "As you can imagine, the examples shown here are just a tip of the iceberg. You can slice and dice the `rules` DataFrame in as many ways as you can imagine. Feel free to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedcc77b",
   "metadata": {},
   "source": [
    "## 4. Evaluate Association Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7e801e",
   "metadata": {},
   "source": [
    "Now that we've created association rules and know how to filter rules based on different criteria, let's take a look at how to evaluate them based on the associated metrics. \n",
    "\n",
    "A quick way to get a big-picture view of the metrics is with summary statistics. We do this by calling the `describe()` method of the `rules` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcad505f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "742b82b6",
   "metadata": {},
   "source": [
    "The summary statistics provide us with the mean, standard deviation, minimum, maximum and some percentile values for the association rule metrics. From the summary, we can tell that a typical rule has a lift of $1.76$ and that the lift values range from $0.99$ to $3.04$.\n",
    "\n",
    "**Lift** tells us how much more the antecedent and consequent occur together in contrast to how often they occur independently. In other words, lift is the strength of association. Lift values range from $0$ to $\\infty$, where a value of $1$ indicates independence between the antecedent and the consequent. Let's take a look at the top $5$ rules by lift: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcb7c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2a92764",
   "metadata": {},
   "source": [
    "The first rule has a lift score of $3.04$. We interpret this to mean that customers who bought beef are $3.04$ times more likely to also purchase root vegetables. Note that lift values above $1$ indicate an increased likelihood, while lift values below $1$ indicate a reduced likelihood.\n",
    "\n",
    "**Leverage** is similar to lift and can be thought of as a normalized value for lift. Leverage values range from $-1$ to $1$, where a value of $0$ indicates independence between the antecedent and the consequent. Let's take a look at the top $5$ rules by leverage: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061d30fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0266875b",
   "metadata": {},
   "source": [
    "The first rule has a leverage score of $0.026$. We interpret this to mean that customers who bought root vegetables are also likely to purchase other vegetables. This is expected behavior. However, the second rule which tells us that customers who bought whole milk are $1.5$ times or $50\\%$ (using the lift scores) more likely to also purchase other vegetables is a bit suspect. Rules that include highly purchased items such as whole milk can be deceiving, so we should also look at the conviction of association rules.\n",
    "\n",
    "**Conviction** quantifies how dependent the consequent is on the antecedent. It is also related to lift. However unlike lift, coviction is sensitive to rule direction. This means that $\\text{Conviction}_{A \\rightarrow B} \\neq \\text{Conviction}_{B \\rightarrow A}$. Conviction values range from $0$ to $\\infty$, where a value of $1$ indicates independence between the antecedent and the consequent. Let's take a look at the top $5$ rules by conviction: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6d941b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd2117f2",
   "metadata": {},
   "source": [
    "The first rule has a conviction of $1.54$. This means that the rule $\\{\\text{tropical fruit, yogurt}\\} \\rightarrow \\{\\text{whole milk}\\}$ would be incorrect $54\\%$ more often (or $1.54$ times as often) if the consequent was independent of the antecedent. The higher the conviction, the more likely it is that the consequent is dependent on the antecedent.\n",
    "\n",
    "Besides the metrics returned by the `association_rules` function, **Zhang's Metric** is another useful metric that we should also take into consideration when evaluating rules. It ranges in value from $-1$ to $1$ which represent perfect association and perfect dissociation respectively. Zhang's metric is useful in identifying items that should not be placed next to each other, even if they have been purchased together previously. It is calculated as follows:\n",
    "\n",
    "$$ \\text{Zhang}_{A \\rightarrow B} = \\frac{\\text{Support}_{A \\rightarrow B} - (\\text{Support}_{A} \\times \\text{Support}_{B})}{\\text{max}\\{[\\text{Support}_{A \\rightarrow B} \\times (1 - \\text{Support}_{A})], [\\text{Support}_{A} \\times (\\text{Support}_{B} - \\text{Support}_{A \\rightarrow B})]\\}}$$\n",
    "\n",
    "Where $\\text{Support}_{A \\rightarrow B}$ is the support of the rule, $\\text{Support}_{A}$ is the antecedent support and $\\text{Support}_{B}$ is the consequent support.\n",
    "\n",
    "We can add Zhang's metric to our `rules` DataFrame by first creating a function that calculates it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb5edc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81ec8cc2",
   "metadata": {},
   "source": [
    "Then, we assign the result of the function to new column called `'zhang'` in the `rules` DataFrame as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dbfa42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d928c083",
   "metadata": {},
   "source": [
    "Let's take a look at the top $5$ rules by the zhang metric: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef8b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "541565a5",
   "metadata": {},
   "source": [
    "The first rule has a zhang metric score of $0.708$. This indicates a pretty strong positive association between beef and root vegetables. This tells us that if we were to separate beef from root vegetables in our store, there could be an impact to how much of both are purchased. In other words, pairing beef and root vegetables for promotional purposes is a good choice.\n",
    "\n",
    "Looking at rules that have a low zhang metric is also very useful. Let's take a look at the bottom $5$ rules by the zhang metric: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c046f775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a62c207d",
   "metadata": {},
   "source": [
    "The first rule has a zhang metric score of $-0.007$. This indicates a slight dissociation between bottled beer and whole milk. This tells us that if we were to separate bottled beer from whole milk in the store, there would likely not be an appreciable impact on purchase patterns for both items. This means that it would be a bad choice to pair these two items together for promotional purposes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f946df053fbf2b937619d3c5458e7af74262f9a954d8797ba0b27400bcafe06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
